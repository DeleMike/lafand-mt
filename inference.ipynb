{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/lib/python3.11/site-packages (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder model\n",
    "encoder_session = onnxruntime.InferenceSession('oyto_t5_small_onnx/encoder_model.onnx')\n",
    "\n",
    "# Load decoder model\n",
    "decoder_session = onnxruntime.InferenceSession('oyto_t5_small_onnx/decoder_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input\n",
    "def preprocess_input(input_text, tokenizer, max_source_length=1024):\n",
    "    # Prefix to add before the source text\n",
    "    prefix = \"translate undiacritized Yoruba to diacritized Yoruba: \"\n",
    "\n",
    "    # Combine the prefix and input text\n",
    "    input_text_with_prefix = prefix + input_text\n",
    "\n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer(\n",
    "        [input_text_with_prefix],\n",
    "        max_length=max_source_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",  # Return NumPy arrays\n",
    "    )\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to postprocess output\n",
    "def postprocess_output(output_sequence, tokenizer):\n",
    "    # Decode the generated output\n",
    "    decoded_output = tokenizer.batch_decode(output_sequence, skip_special_tokens=True)[0]\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for the model\n",
    "model_name_or_path = 'oyot5_small_unyo_dcyo_mix'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data\n",
    "input_data = preprocess_input('mo fe jeun', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on encoder\n",
    "# encoded_context = encoder_session.run(None, {'input_ids': input_data['input_ids']})\n",
    "encoded_context = encoder_session.run(\n",
    "    None,\n",
    "    {\n",
    "        'input_ids': input_data['input_ids'],\n",
    "        'attention_mask': input_data['attention_mask']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.2424832 , -0.1561737 ,  0.33006942, ...,  0.59456974,\n",
      "         -0.40588298,  0.02734715],\n",
      "        [-0.8927029 , -0.35662335,  0.2571173 , ...,  0.46694997,\n",
      "         -0.628342  ,  0.59965885],\n",
      "        [-0.00323499,  0.13572295,  0.00856398, ...,  0.0839715 ,\n",
      "         -0.06416199, -0.14688414],\n",
      "        ...,\n",
      "        [ 0.3754469 ,  0.13625549, -0.00252384, ...,  0.15271264,\n",
      "         -0.31701604,  0.09131931],\n",
      "        [ 0.3754469 ,  0.13625549, -0.00252384, ...,  0.15271264,\n",
      "         -0.31701604,  0.09131931],\n",
      "        [ 0.3754469 ,  0.13625549, -0.00252384, ...,  0.15271264,\n",
      "         -0.31701604,  0.09131931]]], dtype=float32)]\n",
      "{'input_ids': array([[6819, 4997,    3, ...,    0,    0,    0]]), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_context)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Required inputs (['input_ids', 'encoder_hidden_states']) are missing from input feed (['decoder_input_ids', 'encoder_attention_mask']).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run inference on decoder with the encoded context\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# output_sequence = decoder_session.run(None, {'context_vector': encoded_context})\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m output_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_input_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_attention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:216\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_names, input_feed, run_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    Compute the predictions.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m        sess.run([output_name], {input_name: x})\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_feed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_names:\n\u001b[1;32m    218\u001b[0m         output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:198\u001b[0m, in \u001b[0;36mSession._validate_input\u001b[0;34m(self, feed_input_names)\u001b[0m\n\u001b[1;32m    196\u001b[0m         missing_input_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_input_names:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired inputs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_input_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) are missing from input feed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeed_input_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Required inputs (['input_ids', 'encoder_hidden_states']) are missing from input feed (['decoder_input_ids', 'encoder_attention_mask'])."
     ]
    }
   ],
   "source": [
    "# Run inference on decoder with the encoded context\n",
    "# output_sequence = decoder_session.run(None, {'context_vector': encoded_context})\n",
    "output_sequence = decoder_session.run(\n",
    "    None,\n",
    "    {\n",
    "        'decoder_input_ids': encoded_context,\n",
    "        'encoder_attention_mask': input_data['attention_mask'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
